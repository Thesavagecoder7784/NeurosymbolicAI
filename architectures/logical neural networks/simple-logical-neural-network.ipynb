{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Define a simple dataset with logical rules\n# Example: XOR logic function\nX_train = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\ny_train = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n\n# Define the Logical Neural Network model\nclass LogicalNN(nn.Module):\n    def __init__(self):\n        super(LogicalNN, self).__init__()\n        self.fc1 = nn.Linear(2, 4)  # Input layer to hidden layer\n        self.fc2 = nn.Linear(4, 1)  # Hidden layer to output layer\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))  # ReLU activation for hidden layer\n        x = self.sigmoid(self.fc2(x))  # Sigmoid activation for output layer\n        return x\n\n# Initialize the model, loss function, and optimizer\nmodel = LogicalNN()\ncriterion = nn.BCELoss()  # Binary Cross-Entropy loss for binary classification\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    # Forward pass\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Print progress\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n# Test the model on the XOR logic function\nwith torch.no_grad():\n    test_inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n    predictions = model(test_inputs)\n    predicted_classes = (predictions > 0.5).float()\n\n    print(\"\\nPredictions:\")\n    for i, input in enumerate(test_inputs):\n        print(f\"Input: {input.tolist()}, Predicted Output: {predicted_classes[i].item()}\")\n\n# Example usage:\n# The model should learn to approximate the XOR function, where:\n# [0, 0] -> 0\n# [0, 1] -> 1\n# [1, 0] -> 1\n# [1, 1] -> 0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-17T16:14:33.147569Z","iopub.execute_input":"2024-07-17T16:14:33.148015Z","iopub.status.idle":"2024-07-17T16:14:39.779869Z","shell.execute_reply.started":"2024-07-17T16:14:33.147980Z","shell.execute_reply":"2024-07-17T16:14:39.778456Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch [100/1000], Loss: 0.5202\nEpoch [200/1000], Loss: 0.3129\nEpoch [300/1000], Loss: 0.1679\nEpoch [400/1000], Loss: 0.0937\nEpoch [500/1000], Loss: 0.0575\nEpoch [600/1000], Loss: 0.0382\nEpoch [700/1000], Loss: 0.0270\nEpoch [800/1000], Loss: 0.0201\nEpoch [900/1000], Loss: 0.0155\nEpoch [1000/1000], Loss: 0.0123\n\nPredictions:\nInput: [0.0, 0.0], Predicted Output: 0.0\nInput: [0.0, 1.0], Predicted Output: 1.0\nInput: [1.0, 0.0], Predicted Output: 1.0\nInput: [1.0, 1.0], Predicted Output: 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}